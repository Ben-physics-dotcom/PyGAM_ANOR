{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f53f991b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeit\n",
    "import time\n",
    "import math\n",
    "import gc\n",
    "import glob\n",
    "import pickle\n",
    "import statistics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import shap\n",
    "from scipy.special import softmax\n",
    "from statistics import mean\n",
    "import optuna\n",
    "\n",
    "from interpret.glassbox import ExplainableBoostingClassifier\n",
    "\n",
    "from datetime import datetime, date, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "from dask import dataframe as dd\n",
    "from dask.distributed import Client\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as tkr\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import collections\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "\n",
    "from plotly.offline import iplot\n",
    "import chart_studio.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "import csv\n",
    "from itertools import zip_longest\n",
    "import dataframe_image as dfi\n",
    "\n",
    "# from factor_analyzer import FactorAnalyzer\n",
    "# from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity\n",
    "# from factor_analyzer.factor_analyzer import calculate_kmo\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, PowerTransformer, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, ParameterGrid, train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, f1_score, roc_curve, roc_auc_score, confusion_matrix\n",
    "from sklearn.metrics import precision_recall_curve, auc, silhouette_score\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import GridSearchCV,RandomizedSearchCV,train_test_split\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.cluster import KMeans, MeanShift, estimate_bandwidth\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# import feather\n",
    "# import pyreadr\n",
    "\n",
    "# from skopt import BayesSearchCV\n",
    "from pprint import pprint\n",
    "import xgboost as xgb\n",
    "\n",
    "from copy import deepcopy\n",
    "import plotnine\n",
    "\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "# \"magic\" command to make plots show up in the notebook\n",
    "%matplotlib inline\n",
    "\n",
    "pd.options.display.max_rows = 1000\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "\n",
    "pd.set_option(\"display.precision\",2)\n",
    "pd.options.display.float_format = '{:.3f}'.format\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# sns.set_style('darkgrid')\n",
    "sns.set_theme(style = \"whitegrid\", font_scale= 1.5)\n",
    "\n",
    "date_format = \"%Y-%m-%d\"\n",
    "filepath = 'input/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5fb6a39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'arrears'\n",
    "\n",
    "sens_char = {'gender': 'clientdata.demo.gender', # binary\n",
    "             'age': 'clientdata.demo.age_year', # integer\n",
    "             'num_children': 'clientdata.demo.children', # integer (0-10)\n",
    "             'single_parent': 'clientdata.demo.children_singleparent', # binary\n",
    "             'ms_single': 'clientdata.demo.maritalstatus_expand_SINGLE', # binary\n",
    "             'ms_married': 'clientdata.demo.maritalstatus_expand_MARRIED', # binary\n",
    "             'ms_divorced': 'clientdata.demo.maritalstatus_expand_DIVORCED', # binary\n",
    "             'ms_widowed': 'clientdata.demo.maritalstatus_expand_WIDOWED' # binary\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ce8afda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_value_counts(df, column, name):\n",
    "    print(f\"\\n{name} - {column} Distribution\")\n",
    "    print(df[column].value_counts())\n",
    "    print(df[column].value_counts(normalize=True).mul(100).round(1).astype(str) + '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15204def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def null_values(df):\n",
    "        mis_val = df.isnull().sum()\n",
    "        mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        mis_val_table_ren_columns = mis_val_table.rename(\n",
    "        columns = {0 : 'n_missing', 1 : 'perc'})\n",
    "        mis_val_table_ren_columns = mis_val_table_ren_columns[\n",
    "            mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        'perc', ascending=False).round(1)\n",
    "        print (\"Dataframe has \" + str(df.shape[1]) + \" columns.\\n\"\n",
    "            \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "              \" columns that have missing values.\")\n",
    "        mis_val_df = pd.DataFrame(mis_val_table_ren_columns)\n",
    "#         mis_val_df.to_csv('inputs/mis_values.csv')\n",
    "\n",
    "        return mis_val_df\n",
    "\n",
    "\n",
    "def read_RDa(filename, df_name):\n",
    "    result = pyreadr.read_r(filename) # also works for Rds, rda\n",
    "    print(result.keys()) # let's check what objects we got\n",
    "    df = result[df_name] # extract the pandas data frame for object df1\n",
    "\n",
    "    return df\n",
    "\n",
    "def check_intersect(info_df, features_df):\n",
    "\n",
    "    # Checking both datasets fully intersect\n",
    "    uuid_info = list(info_df['uuid'])\n",
    "    uuid_features = list(features_df['uuid'])\n",
    "    print(len(uuid_info))\n",
    "    print(len(uuid_features))\n",
    "\n",
    "    return set(uuid_info) ^ set(uuid_features) # difference\n",
    "\n",
    "\n",
    "'''\n",
    "{'18 - 20': 0 | '21 - 25': 1 | '26 - 30': 2 | '31 - 35': 3 | '36 - 40': 4 | '41 - 45': 5 | '46 - 50': 6 |\n",
    "'51 - 55': 7 | '56 - 60': 8 | '61 - 65': 9 | '66+':10}\n",
    "'''\n",
    "def age_mapping(rating):\n",
    "    if rating <= 20:\n",
    "        return 0\n",
    "    if rating <= 25:\n",
    "        return 1\n",
    "    if rating <= 30:\n",
    "        return 2\n",
    "    if rating <= 35:\n",
    "        return 3\n",
    "    if rating <= 40:\n",
    "        return 4\n",
    "    if rating <= 45:\n",
    "        return 5\n",
    "    if rating <= 50:\n",
    "        return 6\n",
    "    if rating <= 55:\n",
    "        return 7\n",
    "    if rating <= 60:\n",
    "        return 8\n",
    "    if rating <= 65:\n",
    "        return 9\n",
    "    return 10\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0271d6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "{'<25': 0 (young adult) | '>=25': 1 }\n",
    "'''\n",
    "def age_mapping_binary(rating):\n",
    "    if rating < 25:\n",
    "        return 0\n",
    "    if rating >= 25:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "affdf672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map number of children into 4 groups\n",
    "\n",
    "'''\n",
    "{'0 children': 0 | '1-2 children': 1 | 3+ children: 2}\n",
    "'''\n",
    "def children_mapping_3(rating):\n",
    "    if rating <1:\n",
    "        return 0\n",
    "    if rating <3:\n",
    "        return 1\n",
    "    return 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111bc0cc",
   "metadata": {},
   "source": [
    "# Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddde5aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def output_SC_summary(df, name):\n",
    "    # Create summary excel\n",
    "    appended_data = []\n",
    "    for k,v in sens_char_ext.items():\n",
    "        summary = pd.crosstab(df[v], df['arrears']).reset_index()\n",
    "        summary.drop(columns=summary.columns[0], axis=1, inplace=True)\n",
    "        summary.insert (0, 'feature_value', summary.index)\n",
    "        summary.insert(0, 'feature', [k]*len(summary))\n",
    "        summary.insert(4, 'total count', list(df[v].value_counts().sort_index(ascending=True).values))\n",
    "#         summary.insert(4, 'total count', [df[v].value_counts().sort_index(ascending=True)[i] for i in range(len(summary))])\n",
    "        appended_data.append(summary)\n",
    "\n",
    "    # see pd.concat documentation for more info\n",
    "    appended_data = pd.concat(appended_data)\n",
    "    # write DataFrame to an excel sheet\n",
    "    appended_data.to_excel('./outputs/1_SC/' + name + 'SC_summary.xlsx')\n",
    "\n",
    "    return appended_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638d0f6",
   "metadata": {},
   "source": [
    "# Step 4: Prediction Model Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8aa4867",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(df):\n",
    "    \"\"\"\n",
    "    Splits the input dataframe into training and testing sets, returning two versions:\n",
    "      1. One with demographic features removed.\n",
    "      2. One including all features (the 'demo' features are retained).\n",
    "\n",
    "    Assumes the following global variables exist:\n",
    "      - target: The name of the target variable.\n",
    "      - key_featsubgroups: A DataFrame with a 'subgroup' column and a 'list_features' column.\n",
    "        The row where subgroup == 'demo' contains the list of demographic features.\n",
    "\n",
    "    Parameters:\n",
    "      df : DataFrame\n",
    "          The input dataset containing features and the target variable.\n",
    "\n",
    "    Returns:\n",
    "      X_train : DataFrame\n",
    "          Training features with demographic features removed.\n",
    "      X_test : DataFrame\n",
    "          Testing features with demographic features removed.\n",
    "      y_train : Series\n",
    "          Training target values.\n",
    "      y_test : Series\n",
    "          Testing target values.\n",
    "      X_train_demo : DataFrame\n",
    "          Training features with demographic features retained.\n",
    "      X_test_demo : DataFrame\n",
    "          Testing features with demographic features retained.\n",
    "    \"\"\"\n",
    "    # Get the list of demographic features from key_featsubgroups.\n",
    "    demo_feat = list(key_featsubgroups.loc[key_featsubgroups.subgroup == 'demo', 'list_features'])[0]\n",
    "\n",
    "    # Separate features and target.\n",
    "    X = df.drop([target], axis=1)\n",
    "    y = df[target]\n",
    "\n",
    "    # Split the data into training and testing sets (keeping demo features).\n",
    "    X_train_demo, X_test_demo, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=rs)\n",
    "\n",
    "    # Create versions with demo features removed.\n",
    "    X_train = X_train_demo.drop(demo_feat, axis=1)\n",
    "    X_test = X_test_demo.drop(demo_feat, axis=1)\n",
    "\n",
    "    # Print shapes for verification.\n",
    "    print('Training Shape (incl. demo features):', X_train_demo.shape)\n",
    "    print('Training Shape:', X_train.shape)\n",
    "    print('Training Labels Shape:', y_train.shape)\n",
    "\n",
    "    print('Testing Shape (incl. demo features):', X_test_demo.shape)\n",
    "    print('Testing Shape:', X_test.shape)\n",
    "    print('Testing Labels Shape:', y_test.shape)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, X_train_demo, X_test_demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1bf7d620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_pred(X_train, X_test, y_train, y_test, model, name, params, results_dict):\n",
    "    \"\"\"\n",
    "    Trains the given model, evaluates its performance, and stores results.\n",
    "\n",
    "    Parameters:\n",
    "    X_train (DataFrame): Training feature set.\n",
    "    X_test (DataFrame): Testing feature set.\n",
    "    y_train (Series): Training labels.\n",
    "    y_test (Series): Testing labels.\n",
    "    model (sklearn classifier): The machine learning model to be trained.\n",
    "    name (str): The name of the model for result storage.\n",
    "    params (str): Specifies whether the model is using hyperparameter search ('search') or optimized parameters ('opt').\n",
    "    results_dict (dict): Dictionary to store model performance metrics.\n",
    "\n",
    "    Returns:\n",
    "    dict: Updated results dictionary containing model evaluation metrics.\n",
    "    \"\"\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    print(\"\\nStarting model training and evaluation...\")\n",
    "\n",
    "    # Train the model based on parameter selection method\n",
    "    if params == \"search\":\n",
    "        model.fit(X_train, y_train.values.ravel())  # Ensure correct shape for sklearn\n",
    "    elif params == \"opt\":\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "    # Make predictions on train and test sets\n",
    "    train_predictions = model.predict(X_train)\n",
    "    test_predictions = model.predict(X_test)\n",
    "\n",
    "    # Compute confusion matrices\n",
    "    cfm_train = confusion_matrix(y_train, train_predictions)\n",
    "    cfm_test = confusion_matrix(y_test, test_predictions)\n",
    "\n",
    "    # Compute accuracy scores\n",
    "    accs_train = accuracy_score(y_train, train_predictions)\n",
    "    accs_test = accuracy_score(y_test, test_predictions)\n",
    "\n",
    "    # Compute F1-scores for both classes (0 and 1)\n",
    "    f1s_train_p1 = f1_score(y_train, train_predictions, pos_label=1)\n",
    "    f1s_train_p0 = f1_score(y_train, train_predictions, pos_label=0)\n",
    "    f1s_test_p1 = f1_score(y_test, test_predictions, pos_label=1)\n",
    "    f1s_test_p0 = f1_score(y_test, test_predictions, pos_label=0)\n",
    "\n",
    "    # Compute ROC-AUC score for test data\n",
    "    test_ras = roc_auc_score(y_test, model.predict_proba(X_test)[:, 1])\n",
    "\n",
    "    # Extract best hyperparameters\n",
    "    if params == \"search\":\n",
    "        bp = model.best_params_\n",
    "    elif params == \"opt\":\n",
    "        bp = model.get_params()\n",
    "\n",
    "    # Calculate total runtime in minutes\n",
    "    total_time = (time.time() - start_time) / 60\n",
    "    print(f\"Total execution time: {total_time:.2f} minutes\")\n",
    "\n",
    "    # Store computed values in results dictionary (keeping original key names)\n",
    "    results_dict[name] = {\n",
    "        \"classifier\": deepcopy(model),\n",
    "        \"cfm_train\": cfm_train,\n",
    "        \"cfm_test\": cfm_test,\n",
    "        \"train_accuracy\": accs_train,\n",
    "        \"test_accuracy\": accs_test,\n",
    "        \"train F1-score label 1\": f1s_train_p1,\n",
    "        \"train F1-score label 0\": f1s_train_p0,\n",
    "        \"test F1-score label 1\": f1s_test_p1,\n",
    "        \"test F1-score label 0\": f1s_test_p0,\n",
    "        \"test roc auc score\": test_ras,\n",
    "        \"best_params\": bp,\n",
    "        \"time_m\": total_time\n",
    "    }\n",
    "\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975989a6",
   "metadata": {},
   "source": [
    "# Step 4: Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d2154c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc_curve(X_test, y_test, model):\n",
    "    # predict probabilities\n",
    "    probs = model.predict_proba(X_test)\n",
    "    preds = probs[:,1]\n",
    "    # plot no skill roc curve\n",
    "    plt.plot([0, 1], [0, 1], linestyle='--', label='No Skill')\n",
    "    # calculate roc curve for model\n",
    "    fpr, tpr, _ = roc_curve(y_test, preds)\n",
    "    # calculate roc auc\n",
    "    roc_auc = roc_auc_score(y_test, preds)\n",
    "    # plot model roc curve\n",
    "    plt.plot(fpr, tpr, marker='.', label='Logistic AUC = %0.2f' % roc_auc)\n",
    "    # axis labels\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "#     plt.savefig(output_folder + model_name + '_roc_curve.png', bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "#     return preds\n",
    "\n",
    "# plot no skill and model precision-recall curves\n",
    "def plot_pr_curve(y_test, preds):\n",
    "    # calculate the no skill line as the proportion of the positive class\n",
    "    no_skill = len(y_test[y_test==1]) / len(y_test)\n",
    "    # plot the no skill precision-recall curve\n",
    "    plt.plot([0, 1], [no_skill, no_skill], linestyle='--', label='No Skill')\n",
    "    # plot model precision-recall curve\n",
    "    precision, recall, _ = precision_recall_curve(y_test, preds)\n",
    "    auc_score = auc(recall, precision)\n",
    "    plt.plot(recall, precision, marker='.', label='Logistic PR AUC: %.3f' % auc_score)\n",
    "    # axis labels\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    # show the legend\n",
    "    plt.legend()\n",
    "#     plt.savefig(output_folder + model_name + '_pr_curve.png', bbox_inches='tight')\n",
    "    # show the plot\n",
    "    plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Work2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
